{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3075433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Bloc de base : un petit réseau feed-forward résiduel (TinyBlock)\n",
    "# -------------------------------------------------------------------\n",
    "class TinyBlock(layers.Layer):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        # Normalisation couche par couche pour stabiliser les activations\n",
    "        self.ln = layers.LayerNormalization()\n",
    "        # Première couche dense : expansion dimensionnelle + non-linéarité GELU\n",
    "        self.fc1 = layers.Dense(4 * d, activation=\"gelu\")\n",
    "        # Deuxième couche dense : réduction vers la dimension d'origine\n",
    "        self.fc2 = layers.Dense(d)\n",
    "\n",
    "    def call(self, u):\n",
    "        # Normalisation de l'entrée\n",
    "        h = self.ln(u)\n",
    "        # Passage à travers les deux couches denses\n",
    "        h = self.fc1(h)\n",
    "        h = self.fc2(h)\n",
    "        # Résiduel : on ajoute l'entrée initiale à la sortie du bloc\n",
    "        return u + h\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Modèle principal TRM : architecture récurrente inspirée des Transformers\n",
    "# -------------------------------------------------------------------\n",
    "class TRM(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,     # Taille du vocabulaire (pour l'embedding et la sortie)\n",
    "        d=128,          # Dimension du vecteur d'état\n",
    "        max_len=16,     # Longueur maximale des séquences\n",
    "        n_rec=6,        # Nombre d'itérations internes (récurrence fine)\n",
    "        T=3,            # Nombre d'étapes de propagation avant backprop\n",
    "        Nsup=8          # Nombre de cycles supervisés (multi-passes)\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Sauvegarde des hyperparamètres\n",
    "        self.d = d\n",
    "        self.n_rec = n_rec\n",
    "        self.T = T\n",
    "        self.Nsup = Nsup\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # Embeddings : pour encoder les tokens et leurs positions\n",
    "        # -------------------------------------------------------------\n",
    "        self.emb = layers.Embedding(vocab_size, d)\n",
    "        self.pos = self.add_weight(\n",
    "            shape=(1, max_len, d),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "            name=\"positional_embeddings\"\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # États initiaux appris (y0 et z0)\n",
    "        # Ils représentent les états \"mémoire\" de départ (appris globalement)\n",
    "        # -------------------------------------------------------------\n",
    "        self.y0 = self.add_weight(\n",
    "            shape=(1, max_len, d),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            name=\"y0\"\n",
    "        )\n",
    "        self.z0 = self.add_weight(\n",
    "            shape=(1, max_len, d),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            name=\"z0\"\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # Réseau interne partagé : deux TinyBlocks utilisés partout\n",
    "        # -------------------------------------------------------------\n",
    "        self.block1 = TinyBlock(d)\n",
    "        self.block2 = TinyBlock(d)\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # Têtes de sortie :\n",
    "        # - to_vocab : logits sur le vocabulaire (sortie principale)\n",
    "        # - halt_head : probabilité de \"stop\" conditionnelle\n",
    "        # -------------------------------------------------------------\n",
    "        self.to_vocab = layers.Dense(vocab_size)\n",
    "        self.halt_head = layers.Dense(1)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Petit réseau composé de deux TinyBlocks successifs\n",
    "    # -------------------------------------------------------------\n",
    "    def tiny_net(self, u):\n",
    "        u = self.block1(u)\n",
    "        u = self.block2(u)\n",
    "        return u\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Mise à jour des états internes :\n",
    "    # z dépend de x, y et z\n",
    "    # y dépend de y et z\n",
    "    # -------------------------------------------------------------\n",
    "    def update_z(self, x, y, z):\n",
    "        u = x + y + z\n",
    "        return self.tiny_net(u)\n",
    "\n",
    "    def update_y(self, y, z):\n",
    "        u = y + z\n",
    "        return self.tiny_net(u)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Boucle d’entraînement / inférence principale\n",
    "    # -------------------------------------------------------------\n",
    "    def call(self, x_tokens, y_true=None, training=False):\n",
    "        B = tf.shape(x_tokens)[0]  # Taille du batch\n",
    "        L = tf.shape(x_tokens)[1]  # Longueur de séquence\n",
    "\n",
    "        # Embedding des tokens + ajout de l'encodage positionnel\n",
    "        x = self.emb(x_tokens) + self.pos[:, :L, :]\n",
    "\n",
    "        # Initialisation des états y et z à partir des tenseurs appris\n",
    "        y = tf.tile(self.y0[:, :L, :], [B, 1, 1])\n",
    "        z = tf.tile(self.z0[:, :L, :], [B, 1, 1])\n",
    "\n",
    "        losses = []  # Stocke les pertes à chaque passe supervisée\n",
    "\n",
    "        # -----------------------------------------------------------------\n",
    "        # Boucle principale : Nsup cycles de raffinement successif\n",
    "        # -----------------------------------------------------------------\n",
    "        for step in range(self.Nsup):\n",
    "\n",
    "            # Chaque cycle comporte T étapes, la dernière reçoit un gradient\n",
    "            for t in range(self.T):\n",
    "\n",
    "                if t < self.T - 1:\n",
    "                    # --- Étapes sans gradient ---\n",
    "                    # Permet au modèle de \"réfléchir\" sans affecter les poids\n",
    "                    for _ in range(self.n_rec):\n",
    "                        z = tf.stop_gradient(self.update_z(x, y, z))\n",
    "                    y = tf.stop_gradient(self.update_y(y, z))\n",
    "                else:\n",
    "                    # --- Dernière étape avec gradient ---\n",
    "                    # C’est ici que la rétropropagation agit\n",
    "                    for _ in range(self.n_rec):\n",
    "                        z = self.update_z(x, y, z)\n",
    "                    y = self.update_y(y, z)\n",
    "\n",
    "            # Calcul des logits et de la probabilité d'arrêt\n",
    "            logits = self.to_vocab(y)\n",
    "            halt_p = tf.sigmoid(tf.reduce_mean(self.halt_head(y), axis=1))\n",
    "\n",
    "            # -------------------------------------------------------------\n",
    "            # Si sortie supervisée (entraînement)\n",
    "            # Calcul de la perte cross-entropy + halt loss\n",
    "            # -------------------------------------------------------------\n",
    "            if y_true is not None:\n",
    "                # Cross entropy sur le vocabulaire\n",
    "                ce = tf.reduce_mean(\n",
    "                    tf.keras.losses.sparse_categorical_crossentropy(\n",
    "                        y_true, logits, from_logits=True\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # Prédictions correctes ou non\n",
    "                pred = tf.argmax(logits, axis=-1)\n",
    "                correct = tf.reduce_all(tf.equal(pred, y_true), axis=1)\n",
    "                correct = tf.cast(correct, tf.float32)\n",
    "                correct = tf.expand_dims(correct, axis=1)\n",
    "\n",
    "                # Binary cross entropy pour la tête \"halt\"\n",
    "                bce = tf.reduce_mean(\n",
    "                    tf.keras.losses.binary_crossentropy(correct, halt_p)\n",
    "                )\n",
    "\n",
    "                # Perte totale pour cette passe\n",
    "                losses.append(ce + 0.5 * bce)\n",
    "\n",
    "            # Détachement des états pour le cycle suivant\n",
    "            y = tf.stop_gradient(y)\n",
    "            z = tf.stop_gradient(z)\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # Sortie :\n",
    "        # - Si pas de labels : logits + probabilité d'arrêt\n",
    "        # - Sinon perte moyenne sur toutes les passes\n",
    "        # -------------------------------------------------------------\n",
    "        if y_true is None:\n",
    "            return logits, halt_p\n",
    "\n",
    "        return tf.add_n(losses) / len(losses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
